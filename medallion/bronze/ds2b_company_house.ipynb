{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa8e838d-998b-49e5-ac23-acf70a9b6956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Bronze Layer – Companies House ETL Pipeline\n",
    "\n",
    "### Objective\n",
    "\n",
    "The purpose of the Bronze layer in this pipeline is to **ingest raw Companies House API data and store it in a structured, queryable Delta format** while preserving the original source fidelity. This layer focuses on **data capture, traceability, and scalability**, not business transformations.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Source\n",
    "\n",
    "* Source System: **UK Companies House API**\n",
    "* Data Type: **JSON (multiline, nested)**\n",
    "* Datasets Ingested:\n",
    "\n",
    "  * Company Overview\n",
    "  * Officers\n",
    "  * Filing History\n",
    "\n",
    "Each company’s data arrives as nested JSON files organised in a folder hierarchy based on **year / month / day / company_number**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "#### 1. Raw to Delta Conversion\n",
    "\n",
    "Raw JSON files stored in Databricks Volumes are converted into **Delta Tables**.\n",
    "Delta format provides:\n",
    "\n",
    "* ACID transactions\n",
    "* Schema enforcement\n",
    "* Time travel\n",
    "* Performance optimisation\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Explicit Schema Definition\n",
    "\n",
    "Instead of relying on schema inference, **StructType schemas** were defined manually for each dataset.\n",
    "This prevents:\n",
    "\n",
    "* `_corrupt_record` issues\n",
    "* Schema drift\n",
    "* Unexpected data type mismatches\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Multiline JSON Handling\n",
    "\n",
    "The Companies House API returns nested, multiline JSON.\n",
    "Spark was configured using:\n",
    "\n",
    "```\n",
    ".option(\"multiline\", \"true\")\n",
    "```\n",
    "\n",
    "to correctly parse large JSON objects.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Metadata Extraction from File Path\n",
    "\n",
    "Unity Catalog does not support `input_file_name()`, so `_metadata.file_path` was used to extract:\n",
    "\n",
    "* `company_number`\n",
    "\n",
    "This ensures every record is **uniquely identifiable and traceable to its source file**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Flattening Nested Structures\n",
    "\n",
    "Some datasets contained arrays (e.g., officers, filing items).\n",
    "These were flattened using:\n",
    "\n",
    "```\n",
    "explode()\n",
    "```\n",
    "\n",
    "This makes the tables query-friendly while still remaining close to the source structure.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Audit Column\n",
    "\n",
    "A technical audit column was added:\n",
    "\n",
    "* `last_updated_ts` → captures table write time\n",
    "\n",
    "This enables:\n",
    "\n",
    "* Refresh tracking\n",
    "* Incremental logic in future layers\n",
    "* Operational monitoring\n",
    "\n",
    "---\n",
    "\n",
    "### Metadata-Driven Architecture\n",
    "\n",
    "To improve scalability and avoid hardcoding:\n",
    "\n",
    "* A **config.json** file stores catalog, schema, base path, and dataset metadata.\n",
    "* The notebook dynamically reads configuration using **Databricks Widgets**.\n",
    "* This allows easy environment switching (Dev / Test / Prod) without code changes.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Tables (Unity Catalog)\n",
    "\n",
    "Catalog: `companies-data`\n",
    "Schema: `bronze`\n",
    "\n",
    "Tables Created:\n",
    "\n",
    "* `overview`\n",
    "* `officers`\n",
    "* `filing_history`\n",
    "\n",
    "Each table:\n",
    "\n",
    "* Is stored in Delta format\n",
    "* Uses `company_number` as the primary identifier\n",
    "* Includes `last_updated_ts`\n",
    "\n",
    "---\n",
    "\n",
    "### What the Bronze Layer Does NOT Do\n",
    "\n",
    "To maintain proper medallion architecture separation, the Bronze layer intentionally avoids:\n",
    "\n",
    "* Business transformations\n",
    "* Deduplication\n",
    "* Joins\n",
    "* Aggregations\n",
    "* Data cleansing\n",
    "\n",
    "These responsibilities are deferred to the **Silver layer**.\n",
    "\n",
    "---\n",
    "\n",
    "### Result\n",
    "\n",
    "The Bronze layer provides a **reliable, scalable, and audit-ready raw data foundation** that preserves source integrity while enabling downstream analytical processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4909ce13-7028-445f-8b1b-2db0cd654dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\n",
    "    name=\"config_path\",\n",
    "    defaultValue=\"/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/medallion/bronze/config_company_house.json\",\n",
    "    label=\"Config File Path\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c25d9503-77ba-42db-97c0-f00913b8388f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    regexp_extract,\n",
    "    explode,\n",
    "    current_timestamp\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# IMPORT SHARED UTILITIES\n",
    "# =========================\n",
    "from utils.logger import get_logger\n",
    "from utils.sparksession import create_spark_session\n",
    "from utils.schema import SCHEMA_MAP\n",
    "\n",
    "# =========================\n",
    "# INITIALISE LOGGER & SPARK\n",
    "# =========================\n",
    "logger = get_logger(\"ds2b_company_house_bronze\")\n",
    "spark = create_spark_session(\"DS2B | Company House Bronze\")\n",
    "\n",
    "logger.info(\"Spark session created successfully\")\n",
    "\n",
    "# =========================\n",
    "# LOAD CONFIG\n",
    "# =========================\n",
    "config_path = dbutils.widgets.get(\"config_path\")\n",
    "logger.info(f\"Loading config from: {config_path}\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "CATALOG = config[\"catalog\"]\n",
    "SCHEMA = config[\"schema\"]\n",
    "BASE_PATH = config[\"base_path\"]\n",
    "\n",
    "logger.info(\n",
    "    f\"Config loaded | Catalog={CATALOG}, Schema={SCHEMA}, BasePath={BASE_PATH}\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# PROCESS TABLES\n",
    "# =========================\n",
    "for table in config[\"tables\"]:\n",
    "\n",
    "    table_name = table[\"name\"]\n",
    "    file_name = table[\"file\"]\n",
    "    explode_flag = table.get(\"explode\", False)\n",
    "    explode_column = table.get(\"explode_column\")\n",
    "\n",
    "    logger.info(f\"Starting processing for table: {table_name}\")\n",
    "\n",
    "    df = (\n",
    "        spark.read\n",
    "        .schema(SCHEMA_MAP[table_name])\n",
    "        .option(\"multiline\", \"true\")\n",
    "        .json(f\"{BASE_PATH}/*/*/*/*/{file_name}\")\n",
    "        .withColumn(\"file_path\", col(\"_metadata.file_path\"))\n",
    "        .withColumn(\n",
    "            \"company_number\",\n",
    "            regexp_extract(\"file_path\", r'/([0-9A-Z]+)/[^/]+$', 1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Read completed for {table_name}\")\n",
    "\n",
    "    if explode_flag:\n",
    "        logger.info(\n",
    "            f\"Exploding column '{explode_column}' for table {table_name}\"\n",
    "        )\n",
    "        df = (\n",
    "            df.withColumn(\"exploded\", explode(explode_column))\n",
    "              .select(\"company_number\", \"exploded.*\")\n",
    "        )\n",
    "\n",
    "    df = df.withColumn(\"last_updated_ts\", current_timestamp())\n",
    "\n",
    "    logger.info(f\"Writing Delta table: {CATALOG}.{SCHEMA}.{table_name}\")\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(f\"`{CATALOG}`.{SCHEMA}.{table_name}\")\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Completed table: {table_name}\")\n",
    "\n",
    "logger.info(\"Metadata-Driven Bronze Pipeline Completed Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c993af0-d088-4f12-9b7e-543be1304657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ds2b_company_house",
   "widgets": {
    "config_path": {
     "currentValue": "/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/medallion/bronze/config_company_house.json",
     "nuid": "a9d25557-5682-4e79-9e8b-ba20038e6ade",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/databricks/bronze/config_company_house.json",
      "label": "Config File Path",
      "name": "config_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/databricks/bronze/config_company_house.json",
      "label": "Config File Path",
      "name": "config_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
