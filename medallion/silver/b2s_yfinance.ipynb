{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af8646a-ff18-4bb8-ba61-645d1d3d4f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# WIDGET\n",
    "# ==============================\n",
    "dbutils.widgets.text(\n",
    "    name=\"config_path\",\n",
    "    defaultValue=\"/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/medallion/silver/config_yfinance.json\",\n",
    "    label=\"Config File Path\"\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# IMPORTS\n",
    "# =========================\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, BooleanType, StringType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from utils.logger import get_logger\n",
    "from utils.sparksession import create_spark_session\n",
    "\n",
    "# =========================\n",
    "# INIT LOGGER & SPARK\n",
    "# =========================\n",
    "logger = get_logger(\"b2s_yfinance_scd2_safe\")\n",
    "spark = create_spark_session(\"B2S | YFinance SCD2 SAFE\")\n",
    "\n",
    "# =========================\n",
    "# LOAD CONFIG\n",
    "# =========================\n",
    "config_path = dbutils.widgets.get(\"config_path\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "CATALOG = config[\"catalog\"]\n",
    "BRONZE_SCHEMA = config[\"bronze_schema\"]\n",
    "SILVER_SCHEMA = config[\"silver_schema\"]\n",
    "TABLES = config[\"tables\"]\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def table_exists(table_name: str) -> bool:\n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def ensure_scd_columns(table_name: str):\n",
    "    \"\"\"\n",
    "    Add missing SCD columns safely\n",
    "    \"\"\"\n",
    "    existing_cols = {c.name for c in spark.table(table_name).schema}\n",
    "\n",
    "    alter_stmts = []\n",
    "\n",
    "    if \"row_hash\" not in existing_cols:\n",
    "        alter_stmts.append(\"ADD COLUMN row_hash STRING\")\n",
    "\n",
    "    if \"effective_from\" not in existing_cols:\n",
    "        alter_stmts.append(\"ADD COLUMN effective_from DATE\")\n",
    "\n",
    "    if \"effective_to\" not in existing_cols:\n",
    "        alter_stmts.append(\"ADD COLUMN effective_to DATE\")\n",
    "\n",
    "    if \"is_current\" not in existing_cols:\n",
    "        alter_stmts.append(\"ADD COLUMN is_current BOOLEAN\")\n",
    "\n",
    "    if alter_stmts:\n",
    "        stmt = f\"ALTER TABLE {table_name} \" + \" \".join(alter_stmts)\n",
    "        spark.sql(stmt)\n",
    "        logger.info(f\"Added missing SCD columns to {table_name}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PROCESS TABLES\n",
    "# =========================\n",
    "for table in TABLES:\n",
    "\n",
    "    table_name = table[\"name\"]\n",
    "    business_keys = table[\"business_key\"]\n",
    "    tracked_columns = table[\"tracked_columns\"]\n",
    "    hash_column = table[\"hash_column\"]\n",
    "\n",
    "    logger.info(f\"Processing table: {table_name}\")\n",
    "\n",
    "    bronze_df = spark.table(f\"`{CATALOG}`.{BRONZE_SCHEMA}.{table_name}\")\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # HASH\n",
    "    # -----------------------------------------\n",
    "    bronze_df = bronze_df.withColumn(\n",
    "        hash_column,\n",
    "        F.sha2(\n",
    "            F.concat_ws(\n",
    "                \"||\",\n",
    "                *[F.col(c).cast(\"string\") for c in tracked_columns]\n",
    "            ),\n",
    "            256\n",
    "        )\n",
    "    )\n",
    "\n",
    "    target_table = f\"`{CATALOG}`.{SILVER_SCHEMA}.{table_name}\"\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # FIRST RUN → CREATE SILVER TABLE\n",
    "    # -----------------------------------------\n",
    "    if not table_exists(target_table):\n",
    "        logger.info(f\"Creating Silver table: {target_table}\")\n",
    "\n",
    "        (\n",
    "            bronze_df\n",
    "            .withColumn(\"effective_from\", F.current_date())\n",
    "            .withColumn(\"effective_to\", F.lit(None).cast(DateType()))\n",
    "            .withColumn(\"is_current\", F.lit(True))\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(target_table)\n",
    "        )\n",
    "\n",
    "        continue  # move to next table\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # ENSURE SCD COLUMNS EXIST\n",
    "    # -----------------------------------------\n",
    "    ensure_scd_columns(target_table)\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, target_table)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # BUILD MERGE CONDITION (COMPOSITE KEYS)\n",
    "    # -----------------------------------------\n",
    "    merge_condition = \" AND \".join(\n",
    "        [f\"t.{k} = s.{k}\" for k in business_keys]\n",
    "    ) + \" AND t.is_current = true\"\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 1️⃣ EXPIRE CHANGED ROWS\n",
    "    # -----------------------------------------\n",
    "    delta_table.alias(\"t\") \\\n",
    "        .merge(\n",
    "            bronze_df.alias(\"s\"),\n",
    "            merge_condition\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(\n",
    "            condition=f\"t.{hash_column} <> s.{hash_column}\",\n",
    "            set={\n",
    "                \"effective_to\": \"current_date()\",\n",
    "                \"is_current\": \"false\"\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 2️⃣ INSERT NEW + CHANGED ROWS\n",
    "    # -----------------------------------------\n",
    "    silver_df = spark.table(target_table)\n",
    "\n",
    "    join_expr = [\n",
    "        bronze_df[k] == silver_df[k] for k in business_keys\n",
    "    ] + [silver_df.is_current == True]\n",
    "\n",
    "    new_rows_df = (\n",
    "        bronze_df.alias(\"s\")\n",
    "        .join(\n",
    "            silver_df.alias(\"t\"),\n",
    "            join_expr,\n",
    "            \"left_anti\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    (\n",
    "        new_rows_df\n",
    "        .withColumn(\"effective_from\", F.current_date())\n",
    "        .withColumn(\"effective_to\", F.lit(None).cast(DateType()))\n",
    "        .withColumn(\"is_current\", F.lit(True))\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(target_table)\n",
    "    )\n",
    "\n",
    "    logger.info(f\"SCD2 completed for {table_name}\")\n",
    "\n",
    "logger.info(\"B2S YFinance SCD2 pipeline completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cd6ec31-4d04-4908-9957-d7e3d58936e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "b2s_yfinance",
   "widgets": {
    "config_path": {
     "currentValue": "/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/medallion/silver/config_yfinance.json",
     "nuid": "fb9fb3b0-f018-444f-a60a-97fce5df158f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/medallion/silver/config_yfinance.json",
      "label": "Config File Path",
      "name": "config_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/medallion/silver/config_yfinance.json",
      "label": "Config File Path",
      "name": "config_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
