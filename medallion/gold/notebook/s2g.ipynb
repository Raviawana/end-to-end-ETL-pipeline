{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c7bcf9-5ed2-4482-90b3-7b41663cc07a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# WIDGET\n",
    "# =====================================\n",
    "dbutils.widgets.text(\n",
    "    name=\"config_path\",\n",
    "    defaultValue=\"/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/medallion/gold/config.json\",\n",
    "    label=\"Config File Path\"\n",
    ")\n",
    "\n",
    "# =====================================\n",
    "# IMPORTS\n",
    "# =====================================\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from utils.logger import get_logger\n",
    "from utils.sparksession import create_spark_session\n",
    "\n",
    "# =====================================\n",
    "# INIT LOGGER & SPARK\n",
    "# =====================================\n",
    "logger = get_logger(\"s2g_gold_layer\")\n",
    "spark = create_spark_session(\"S2G | Gold Layer\")\n",
    "\n",
    "logger.info(\"Spark session initialised\")\n",
    "\n",
    "# =====================================\n",
    "# LOAD CONFIG\n",
    "# =====================================\n",
    "config_path = dbutils.widgets.get(\"config_path\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "CATALOG = config[\"catalog\"]\n",
    "SILVER = config[\"silver_schema\"]\n",
    "GOLD = config[\"gold_schema\"]\n",
    "\n",
    "PROMOTE_TABLES = config.get(\"promote_tables\", [])\n",
    "DIMENSIONS = config.get(\"dimensions\", [])\n",
    "FACTS = config.get(\"facts\", [])\n",
    "\n",
    "logger.info(f\"Promote tables: {PROMOTE_TABLES}\")\n",
    "logger.info(f\"Dimensions: {[d['name'] for d in DIMENSIONS]}\")\n",
    "logger.info(f\"Facts: {[f['name'] for f in FACTS]}\")\n",
    "\n",
    "# =====================================\n",
    "# UTILS\n",
    "# =====================================\n",
    "def drop_technical_columns(df):\n",
    "    technical_cols = [\"file_path\", \"file_name\", \"last_updated_ts\"]\n",
    "    for c in technical_cols:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(c)\n",
    "    return df\n",
    "\n",
    "# =====================================================\n",
    "# 1️⃣ GENERIC SILVER → GOLD PROMOTION\n",
    "# =====================================================\n",
    "for table in PROMOTE_TABLES:\n",
    "    logger.info(f\"Promoting table: {table}\")\n",
    "\n",
    "    df = spark.table(f\"`{CATALOG}`.{SILVER}.{table}\")\n",
    "    df = drop_technical_columns(df)\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(f\"`{CATALOG}`.{GOLD}.{table}\")\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Promoted table: {table}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2️⃣ DIMENSIONS (LATEST STATE)\n",
    "# =====================================================\n",
    "for dim in DIMENSIONS:\n",
    "    dim_name = dim[\"name\"]\n",
    "    source_table = dim[\"source_table\"]\n",
    "\n",
    "    logger.info(f\"Creating dimension: {dim_name}\")\n",
    "\n",
    "    df = spark.table(f\"`{CATALOG}`.{SILVER}.{source_table}\")\n",
    "\n",
    "    if \"is_current\" in df.columns:\n",
    "        df = df.filter(F.col(\"is_current\") == True)\n",
    "\n",
    "    df = drop_technical_columns(df)\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(f\"`{CATALOG}`.{GOLD}.{dim_name}\")\n",
    "    )\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    OPTIMIZE `{CATALOG}`.{GOLD}.{dim_name}\n",
    "    ZORDER BY (company_number)\n",
    "    \"\"\")\n",
    "\n",
    "# =====================================================\n",
    "# 3️⃣ FACTS + KPIs\n",
    "# =====================================================\n",
    "for fact in FACTS:\n",
    "    fact_name = fact[\"name\"]\n",
    "    source_table = fact[\"source_table\"]\n",
    "    date_col = fact[\"date_column\"]\n",
    "    partition_cols = fact[\"partition_by\"]\n",
    "\n",
    "    logger.info(f\"Creating fact: {fact_name}\")\n",
    "\n",
    "    df = (\n",
    "        spark.table(f\"`{CATALOG}`.{SILVER}.{source_table}\")\n",
    "        .filter(F.col(\"is_current\") == True)\n",
    "    )\n",
    "\n",
    "    df = drop_technical_columns(df)\n",
    "\n",
    "    # KPI logic (only where it makes sense)\n",
    "    if fact_name == \"fact_fundamentals\":\n",
    "        window_q = Window.partitionBy(\"company_number\").orderBy(date_col)\n",
    "\n",
    "        df = (\n",
    "            df.withColumn(\n",
    "                \"revenue_qoq_growth\",\n",
    "                (F.col(\"total_revenue\") - F.lag(\"total_revenue\").over(window_q)) /\n",
    "                F.lag(\"total_revenue\").over(window_q)\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"ebitda_margin\",\n",
    "                F.when(F.col(\"total_revenue\") > 0,\n",
    "                       F.col(\"ebitda\") / F.col(\"total_revenue\"))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(*partition_cols)\n",
    "        .saveAsTable(f\"`{CATALOG}`.{GOLD}.{fact_name}\")\n",
    "    )\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    OPTIMIZE `{CATALOG}`.{GOLD}.{fact_name}\n",
    "    ZORDER BY (company_number)\n",
    "    \"\"\")\n",
    "\n",
    "logger.info(\"S2G Gold pipeline completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "761e4e07-d6c9-4714-852d-b0ac47ff520c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "s2g",
   "widgets": {
    "config_path": {
     "currentValue": "/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/medallion/gold/config.json",
     "nuid": "3f864655-4554-49c7-8722-2b386416d771",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/medallion/gold/config.json",
      "label": "Config File Path",
      "name": "config_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Workspace/Users/ud3041@gmail.com/end-to-end-ETL-pipeline/medallion/gold/config.json",
      "label": "Config File Path",
      "name": "config_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
